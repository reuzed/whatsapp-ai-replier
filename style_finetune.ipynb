{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WhatsApp Sender Style Fine-Tuning (Hugging Face)\n",
        "\n",
        "This brief notebook sketches a minimal, GPU-friendly fine-tuning workflow to teach an open-source model to mimic a specific sender's messaging style.\n",
        "\n",
        "What you'll need (not included here):\n",
        "- A dataset of conversations where the target sender's replies are known. See the Data section below for the expected format and where to plug it in.\n",
        "- A GPU environment (e.g., Runpod) with enough VRAM for the chosen base model.\n",
        "\n",
        "Notes:\n",
        "- Uses QLoRA via `peft` and `trl` for efficient fine-tuning.\n",
        "- Defaults to a small chat model; feel free to switch to a larger one if you have resources.\n",
        "- Be mindful of privacy, TOS, and consent when training on personal chats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -q transformers datasets accelerate peft trl bitsandbytes einops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "# ---- Configuration ----\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    base_model: str = \"HuggingFaceH4/zephyr-7b-beta\"  # change if VRAM limited\n",
        "    out_dir: str = \"./outputs/style-adapter\"\n",
        "    bf16: bool = True\n",
        "    per_device_train_batch_size: int = 2\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    max_steps: int = 500  # keep small for demo; increase for real training\n",
        "    learning_rate: float = 1e-4\n",
        "    warmup_ratio: float = 0.03\n",
        "    logging_steps: int = 10\n",
        "\n",
        "    # QLoRA\n",
        "    lora_r: int = 16\n",
        "    lora_alpha: int = 32\n",
        "    lora_dropout: float = 0.05\n",
        "\n",
        "    # Tokenization/formatting\n",
        "    max_seq_len: int = 1024\n",
        "\n",
        "cfg = TrainConfig()\n",
        "\n",
        "# 4-bit quantization for QLoRA\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if cfg.bf16 else torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "device_map = \"auto\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.base_model, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    cfg.base_model,\n",
        "    device_map=device_map,\n",
        "    quantization_config=bnb_cfg,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "peft_cfg = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=cfg.lora_r,\n",
        "    lora_alpha=cfg.lora_alpha,\n",
        "    lora_dropout=cfg.lora_dropout,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data placeholder (bring your own)\n",
        "\n",
        "You need a dataset of conversations where the target sender's replies are present.\n",
        "A minimal single-turn schema for supervised fine-tuning:\n",
        "\n",
        "- `prompt`: the conversation context and the latest user message, formatted as a single string.\n",
        "- `response`: the exact reply written by the target sender.\n",
        "\n",
        "Example (JSONL) rows you will construct elsewhere:\n",
        "```json\n",
        "{\"prompt\": \"[Alex]: hey are we still on for 7?\", \"response\": \"yeh see u then\"}\n",
        "{\"prompt\": \"[Sam]: send me the doc pls\", \"response\": \"sending now\"}\n",
        "```\n",
        "\n",
        "In multi-turn settings, you can concatenate history; keep sequences under `max_seq_len`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Replace this with real data loading\n",
        "# Expected: a list/dataset with fields: {\"prompt\": str, \"response\": str}\n",
        "\n",
        "examples = [\n",
        "    {\"prompt\": \"[Friend]: wanna grab coffee?\", \"response\": \"down, where?\"},\n",
        "    {\"prompt\": \"[Teammate]: can you review PR #42?\", \"response\": \"on it\"},\n",
        "]\n",
        "\n",
        "dset = Dataset.from_list(examples)\n",
        "\n",
        "BOS = \"\"  # adjust to your model's chat template if needed\n",
        "EOS = tokenizer.eos_token\n",
        "\n",
        "def format_example(rec):\n",
        "    # Basic supervised format: concatenate prompt + response\n",
        "    text = f\"{BOS}{rec['prompt']}\\n{rec['response']}{EOS}\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "train_dset = dset.map(format_example, remove_columns=dset.column_names)\n",
        "\n",
        "print(train_dset[0][\"text\"][:200])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = SFTConfig(\n",
        "    output_dir=cfg.out_dir,\n",
        "    bf16=cfg.bf16,\n",
        "    per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
        "    max_steps=cfg.max_steps,\n",
        "    learning_rate=cfg.learning_rate,\n",
        "    warmup_ratio=cfg.warmup_ratio,\n",
        "    logging_steps=cfg.logging_steps,\n",
        "    save_steps=0,\n",
        "    eval_strategy=\"no\",\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=True,\n",
        "    max_seq_length=cfg.max_seq_len,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dset,\n",
        "    peft_config=peft_cfg,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA adapter\n",
        "os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "trainer.model.save_pretrained(cfg.out_dir)\n",
        "tokenizer.save_pretrained(cfg.out_dir)\n",
        "print(f\"Saved adapter to {cfg.out_dir}\")\n",
        "\n",
        "# Quick inference demo (merges LoRA on the fly)\n",
        "from peft import PeftModel\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    cfg.base_model,\n",
        "    device_map=device_map,\n",
        "    quantization_config=bnb_cfg,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "adapted = PeftModel.from_pretrained(base, cfg.out_dir)\n",
        "adapted.eval()\n",
        "\n",
        "prompt = \"[Alex]: do you want to play tennis later?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(adapted.device)\n",
        "with torch.no_grad():\n",
        "    out = adapted.generate(**inputs, max_new_tokens=40, do_sample=True, temperature=0.7)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
